{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vibhav Jain.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "H80bj59qKyXR"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "import os\n",
        "print(os.listdir(\"../input\"))\n",
        "# Any results you write to the current directory are saved as output.\n",
        "from pandas import read_csv\n",
        "#Lets load the dataset and sample some\n",
        "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
        "data = read_csv('../input/housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\n",
        "print(data.head(5))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX4K7eaDh54s"
      },
      "source": [
        "\n",
        "# Dimension of the dataset\n",
        "print(np.shape(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XJLGsC-iIyc"
      },
      "source": [
        "# Let's summarize the data to see the distribution of data\n",
        "print(data.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUe6HA-iiNTk"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "fig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\n",
        "index = 0\n",
        "axs = axs.flatten()\n",
        "for k,v in data.items():\n",
        "    sns.boxplot(y=k, data=data, ax=axs[index])\n",
        "    index += 1\n",
        "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)\n",
        "\n",
        "\n",
        " for k, v in data.items():\n",
        "        q1 = v.quantile(0.25)\n",
        "        q3 = v.quantile(0.75)\n",
        "        irq = q3 - q1\n",
        "        v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n",
        "        perc = np.shape(v_col)[0] * 100.0 / np.shape(data)[0]\n",
        "        print(\"Column %s outliers = %.2f%%\" % (k, perc))\n",
        "\n",
        "\n",
        "data = data[~(data['MEDV'] >= 50.0)]\n",
        "print(np.shape(data))\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\n",
        "index = 0\n",
        "axs = axs.flatten()\n",
        "for k,v in data.items():\n",
        "    sns.distplot(v, ax=axs[index])\n",
        "    index += 1\n",
        "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "sns.heatmap(data.corr().abs(),  annot=True)\n",
        "\n",
        "\n",
        "\n",
        "from sklearn import preprocessing\n",
        "# Let's scale the columns before plotting them against MEDV\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "column_sels = ['LSTAT', 'INDUS', 'NOX', 'PTRATIO', 'RM', 'TAX', 'DIS', 'AGE']\n",
        "x = data.loc[:,column_sels]\n",
        "y = data['MEDV']\n",
        "x = pd.DataFrame(data=min_max_scaler.fit_transform(x), columns=column_sels)\n",
        "fig, axs = plt.subplots(ncols=4, nrows=2, figsize=(20, 10))\n",
        "index = 0\n",
        "axs = axs.flatten()\n",
        "for i, k in enumerate(column_sels):\n",
        "    sns.regplot(y=y, x=x[k], ax=axs[i])\n",
        "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)\n",
        "\n",
        "\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "\n",
        "y =  np.log1p(y)\n",
        "for col in x.columns:\n",
        "    if np.abs(x[col].skew()) > 0.3:\n",
        "        x[col] = np.log1p(x[col])\n",
        "\n",
        "l_regression = linear_model.LinearRegression()\n",
        "kf = KFold(n_splits=10)\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "x_scaled = min_max_scaler.fit_transform(x)\n",
        "scores = cross_val_score(l_regression, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
        "print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
        "\n",
        "scores_map = {}\n",
        "scores_map['LinearRegression'] = scores\n",
        "l_ridge = linear_model.Ridge()\n",
        "scores = cross_val_score(l_ridge, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
        "scores_map['Ridge'] = scores\n",
        "print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
        "\n",
        "# Lets try polinomial regression with L2 with degree for the best fit\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "#for degree in range(2, 6):\n",
        "#    model = make_pipeline(PolynomialFeatures(degree=degree), linear_model.Ridge())\n",
        "#    scores = cross_val_score(model, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
        "#    print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
        "model = make_pipeline(PolynomialFeatures(degree=3), linear_model.Ridge())\n",
        "scores = cross_val_score(model, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
        "scores_map['PolyRidge'] = scores\n",
        "print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
        "#grid_sv = GridSearchCV(svr_rbf, cv=kf, param_grid={\"C\": [1e0, 1e1, 1e2, 1e3], \"gamma\": np.logspace(-2, 2, 5)}, scoring='neg_mean_squared_error')\n",
        "#grid_sv.fit(x_scaled, y)\n",
        "#print(\"Best classifier :\", grid_sv.best_estimator_)\n",
        "scores = cross_val_score(svr_rbf, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
        "scores_map['SVR'] = scores\n",
        "print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gbr = GradientBoostingRegressor(alpha=0.9,learning_rate=0.05, max_depth=2, min_samples_leaf=5, min_samples_split=2, n_estimators=100, random_state=30)\n",
        "#param_grid={'n_estimators':[100, 200], 'learning_rate': [0.1,0.05,0.02], 'max_depth':[2, 4,6], 'min_samples_leaf':[3,5,9]}\n",
        "#grid_sv = GridSearchCV(gbr, cv=kf, param_grid=param_grid, scoring='neg_mean_squared_error')\n",
        "#grid_sv.fit(x_scaled, y)\n",
        "#print(\"Best classifier :\", grid_sv.best_estimator_)\n",
        "scores = cross_val_score(gbr, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
        "scores_map['GradientBoostingRegressor'] = scores\n",
        "print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std("
      ]
    }
  ]
}